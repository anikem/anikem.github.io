<!DOCTYPE HTML>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110663561-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110663561-1');
  </script>

  <!-- Default Statcounter code for My Personal Website https://anikem.github.io/ -->
  <script type="text/javascript">
  var sc_project=12203587;
  var sc_invisible=1;
  var sc_security="67fbccb9";
  var sc_https=1;
  var sc_remove_link=1;
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><img class="statcounter"
  src="https://c.statcounter.com/12203587/0/67fbccb9/1/" alt="Web
  Analytics"></div></noscript>
  <!-- End of Statcounter Code -->

  <title>Aniruddha (Ani) Kembhavi</title>

  <meta name="author" content="Ani Kembhavi">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px solid blue;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p style="text-align:center">
                <name>Aniruddha (Ani) Kembhavi</name>
              </p>
              <br>
              <p>
                I lead <a href="https://prior.allenai.org/" target="_blank">PRIOR</a>, the computer vision team at the <a href="http://allenai.org/" target="_blank">Allen Institute for AI (AI2)</a> in sunny Seattle.
                I am also an Affiliate Associate Professor at the Computer Science & Engineering department at the <a href="https://www.cs.washington.edu/" target="_blank">University of Washington</a>.
              </p>
              <p>
                Prior to AI2, I spent five enjoyable years at Microsoft Bing, building large scale machine learning systems in the Image &amp; Video Search Relevance team.
              </p>
              <p>
                I grew up in Pune, India where I obtained by Bachelors in Engineering at the <a href="http://www.coep.org.in/" target="_blank">College Of Engineering Pune (COEP)</a>.
                I got my Ph.D. at the <a href="http://www.ece.umd.edu/" target="_blank">University of Maryand, College Park</a>
                under the supervision of <a href="http://www.umiacs.umd.edu/~lsd/" target="_blank">Prof. Larry S. Davis</a>.
              </p>
              <p>
                I am particularly interested in research problems at the intersection of vision, language and embodiment.
              </p>
              <p style="text-align:center">
                <a href="mailto:anik@allenai.org">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=JnUevM0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Aniruddha-Kembhavi/2684226">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/anikembhavi">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:200;max-width:200">
              <img style="width:100%;max-width:100%" alt="profile photo" class="circular" src="images/ani.jpg">
            </td>
          </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Hiring</heading>
              <p>
                <a href="https://prior.allenai.org/" target="_blank">PRIOR</a> is looking to hire highly motivated
                <a href="https://boards.greenhouse.io/thealleninstitute/jobs/209509" target="_blank">Research Scientists</a>,
                <a href="https://boards.greenhouse.io/thealleninstitute/jobs/2059893" target="_blank">Research Engineers</a>,
                <a href="https://boards.greenhouse.io/thealleninstitute/jobs/209503" target="_blank">PostDocs</a> and
                <a href="https://boards.greenhouse.io/thealleninstitute/jobs/812158" target="_blank">Pre-Doctoral Candidates</a>.
                <br>
                We consider applications on a rolling basis.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <table class="news">
                  <tr>
                    <td> Oct 2021 </td>
                    <td> 2 papers accepted to Neurips 2021: Container and Advisor.
                    </td>
                  </tr>

                  <tr>
                    <td> Sep 2021 </td>
                    <td> 1 paper accepted to EMNLP 2021: Iconary (oral).
                    </td>
                  </tr>

                  <tr>
                    <td> Aug 2021 </td>
                    <td> 2 papers accepted to ICCV 2021: RobustNav (oral) and GridToPix.
                    </td>
                  </tr>

                  <tr>
                    <td> June 2022 </td>
                    <td> Serving as an Area Chair for ICLR 2022.
                    </td>
                  </tr>

                  <tr>
                    <td> Mar 2021 </td>
                    <td> Serving as an Area Chair for ACL 2021.
                    </td>
                  </tr>

                  <tr>
                    <td> Feb 2021 </td>
                    <td> 3 papers accepted to CVPR 2021: ManipulaTHOR (oral), Rearrangement (oral) and VidSitu.
                    </td>
                  </tr>

                  <tr>
                    <td> Feb 2021 </td>
                    <td> Served as an Area Chair for CVPR 2021.
                    </td>
                  </tr>

                  <tr>
                    <td> Jan 2021 </td>
                    <td> Our <a href="https://openreview.net/forum?id=UuchYL8wSZo">Hide and Seek paper</a> has been accepted as an Oral presentation to ICLR 2021.
                    </td>
                  </tr>

                  <tr>
                    <td> Sep 2020 </td>
                    <td> 2 papers accepted to Neurips 2020:
                      <a href="https://arxiv.org/pdf/2006.09306">Learning from Interaction</a> and
                      <a href="https://arxiv.org/pdf/2006.14769">Supermasks</a>.
                    </td>
                  </tr>

                  <tr>
                    <td> Sep 2020 </td>
                    <td> X-LXMERT has been accepted to EMNLP. Check out the
                      <a href="https://vision-explorer.allenai.org/text_to_image_generation">new demo</a>
                      and
                      <a href="https://prior.allenai.org/projects/x-lxmert">webpage</a>.
                    </td>
                  </tr>

                  <tr>
                    <td> Aug 2020 </td>
                    <td> AllenAct (BETA) has been released! Check out the
                      <a href="http://allenact.org/">webpage</a> and
                      <a href="https://github.com/allenai/allenact">github</a>
                      repo.

                    </td>
                  </tr>

                  <tr>
                    <td> Apr 2020 </td>
                    <td> 2 papers accepted to ECCV 2020 (Both Spotlights):
                    <a href="https://arxiv.org/pdf/2007.04979">CordialSYNC</a> and
                    <a href="https://arxiv.org/pdf/2003.12058">Grounded Situations</a>
                    </td>
                  </tr>

                  <tr>
                    <td> Apr 2020 </td>
                    <td> 2 papers accepted to CVPR 2020:
                    <a href="https://arxiv.org/pdf/2004.06799.pdf">RoboTHOR</a> and
                    <a href="https://arxiv.org/pdf/1911.13299.pdf">Hidden Networks</a>
                    </td>
                  </tr>

              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <p></p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td><p><heading>Research</heading></td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/container.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2106.01401.pdf"><papertitle>Container: Context Aggregation Networks</papertitle><a/>
          		<br>
          		<paperauthors>Gao Peng, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>Neurips 2021</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2106.01401.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/advisor.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2007.12173"><papertitle>Bridging the Imitation Gap by Adaptive Insubordination</papertitle><a/>
          		<br>
          		<paperauthors>Luca Weihs, Unnat Jain, Iou-Jen Liu, Jordi Salvador, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing</paperauthors>
          		<br>
          		<papervenue>Neurips 2021</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2007.12173">PDF</a>
                <a href="https://unnat.github.io/advisor/"> / Project Page</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/robustnav.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://prior.allenai.org/projects/robustnav"><papertitle>RobustNav : Towards Benchmarking Robustness in Embodied Navigation</papertitle><a/>
          		<br>
          		<paperauthors>Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>ICCV 2021 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/gridtopix.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://unnat.github.io/gridtopix/"><papertitle>GridToPix: Training Embodied Agents with Minimal Supervision</papertitle><a/>
          		<br>
          		<paperauthors>Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik, Aniruddha Kembhavi, Luca Weihs, Alexander Schwing</paperauthors>
          		<br>
          		<papervenue>ICCV 2021</papervenue>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/piglet.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://rowanzellers.com/piglet/"><papertitle>PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World</papertitle><a/>
          		<br>
          		<paperauthors>Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi</paperauthors>
          		<br>
          		<papervenue>ACL 2021 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/manipulathor.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://ai2thor.allenai.org/manipulathor"><papertitle>ManipulaTHOR: A Framework for Visual Object Manipulation</papertitle><a/>
          		<br>
          		<paperauthors>Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi</paperauthors>
          		<br>
          		<papervenue>CVPR 2021 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/rearrangement.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://ai2thor.allenai.org/rearrangement/"><papertitle>Visual Room Rearrangement</papertitle><a/>
          		<br>
          		<paperauthors>Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi</paperauthors>
          		<br>
          		<papervenue>CVPR 2021 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/vidsitu.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://vidsitu.org/"><papertitle>Visual Semantic Role Labeling for Video Understanding</papertitle><a/>
          		<br>
          		<paperauthors>Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>CVPR 2021</papervenue>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/hideseek.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://openreview.net/forum?id=UuchYL8wSZo"><papertitle>Learning Generalizable Visual Representations via Interactive Gameplay</papertitle><a/>
          		<br>
          		<paperauthors>Luca Weihs, Aniruddha Kembhavi, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, & Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>ICLR 2021 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          		<p>
          			<a href="https://openreview.net/pdf?id=UuchYL8wSZo">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/x-lxmert.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2009.11278"><papertitle>X-LXMERT: Paint, Caption and Answer Questionswith Multi-Modal Transformers</papertitle><a/>
          		<br>
          		<paperauthors>Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>EMNLP 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2009.11278">PDF</a>
          			<a href="https://prior.allenai.org/projects/x-lxmert"> / Project Page</a>
                <a href="https://vision-explorer.allenai.org/text_to_image_generation"> / Demo</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/learningobjects.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2006.09306"><papertitle>Learning About Objects by Learning to Interact with Them</papertitle><a/>
          		<br>
          		<paperauthors>Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi</paperauthors>
          		<br>
          		<papervenue>Neurips 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2006.09306">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/supsup.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2006.14769"><papertitle>Supermasks in Superposition</papertitle><a/>
          		<br>
          		<paperauthors>Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>Neurips 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2006.14769">PDF</a>
                <a href="https://mitchellnw.github.io/blog/2020/supsup/"> / Blog post</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/allenact.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2008.12760"><papertitle>AllenAct: A Framework for Embodied AI Research</papertitle><a/>
          		<br>
          		<paperauthors>Luca Weihs, Jordi Salvador, Klemen Kotar, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>Arxiv 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2008.12760">PDF</a>
          			<a href="http://allenact.org/"> / Project Page</a>
                <a href="https://github.com/allenai/allenact"> / Code</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/cordialsync.gif">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2007.04979"><papertitle>A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks</papertitle><a/>
          		<br>
          		<paperauthors>Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing</paperauthors>
          		<br>
          		<papervenue>ECCV 2020 &nbsp;<span class='special'>[Spotlight Presentation]</span></papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2007.04979">PDF</a>
          			<a href="https://unnat.github.io/cordial-sync"> / Project Page</a>
                <a href="https://github.com/allenai/cordial-sync"> / Code</a>
                <a href="https://youtu.be/EnYwCX5E9gk"> / Teaser Video</a>
                <a href="https://youtu.be/xM3auncA06A"> / Detailed Video</a>

          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/gsr.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2003.12058"><papertitle>Grounded Situation Recognition</papertitle><a/>
          		<br>
          		<paperauthors>Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>ECCV 2020 &nbsp;<span class='special'>[Spotlight Presentation]</span></papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2003.12058">PDF</a>
          			<a href="https://prior.allenai.org/projects/gsr"> / Project Page</a>
                <a href="https://github.com/allenai/swig"> / Code</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/ned.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2007.02519"><papertitle>In the Wild: From ML Models to Pragmatic ML Systems</papertitle><a/>
          		<br>
          		<paperauthors>Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron Walsman, Aniruddha Kembhavi, Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>Arxiv 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2007.02519">PDF</a>
                <a href="https://raivn.cs.washington.edu/projects/InTheWild/"> / Project Page</a>
                <a href="https://github.com/RAIVNLab/InTheWild"> / Code</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/objectnav.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2006.13171"><papertitle>ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects</papertitle><a/>
          		<br>
          		<paperauthors>Dhruv Batra, Aaron Gokaslan, Ani Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans</paperauthors>
          		<br>
          		<papervenue>Arxiv 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2006.13171">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/robothor.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/2004.06799.pdf"><papertitle>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</papertitle><a/>
          		<br>
          		<paperauthors>Matt Deitke*, Winson Han*, Alvaro Herrasti*, Aniruddha Kembhavi*, Eric Kolve*, Roozbeh Mottaghi*, Jordi Salvador*, Dustin Schwenk*, Eli VanderBilt*, Matthew Wallingford*, Luca Weihs*, Mark Yatskar*, Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>CVPR 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/2004.06799.pdf">PDF</a>
          			<a href="https://ai2thor.allenai.org/robothor/"> / Project Page</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/hidden.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/1911.13299.pdf"><papertitle>Whatâ€™s Hidden in a Randomly Weighted Neural Network?</papertitle><a/>
          		<br>
          		<paperauthors>Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari</paperauthors>
          		<br>
          		<papervenue>CVPR 2020</papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/1911.13299.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/twobody.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/1904.05879.pdf"><papertitle>Two Body Problem: Collaborative Visual Task Completion</papertitle><a/>
          		<br>
          		<paperauthors>Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander Schwing, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>CVPR 2019 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/1904.05879.pdf">PDF</a>
          			<a href="https://prior.allenai.org/projects/two-body-problem"> / Project Page</a>
          			<a href="https://www.youtube.com/watch?v=VWqawSuH-eU"> / Video</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/elastic.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/pdf/1812.05262.pdf"><papertitle>ELASTIC: Improving CNNs with Instance Specific Scaling Policies</papertitle><a/>
          		<br>
          		<paperauthors>Huiyu Wang, Aniruddha Kembhavi, Ali Farhadi, Alan Yuille, Mohammad Rastegari</paperauthors>
          		<br>
          		<papervenue>CVPR 2019 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          		<p>
          			<a href="https://arxiv.org/pdf/1812.05262.pdf">PDF</a>
          			<a href="https://prior.allenai.org/projects/elastic"> / Project Page</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/flintstones.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1804.03608"><papertitle>Imagine This! Scripts to Compositions to Videos</papertitle><a/>
          		<br>
          		<paperauthors>Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>ECCV 2018</papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1804.03608">PDF</a>
          			<a href="https://prior.allenai.org/projects/craft"> / Project Page</a>
          			<a href="https://www.youtube.com/watch?v=688Vv86n0z8"> / Video</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/iqa.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1712.03316"><papertitle>IQA: Visual Question Answering in Interactive Environments</papertitle><a/>
          		<br>
          		<paperauthors>Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>CVPR 2018</papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1712.03316">PDF</a>
          			<a href="https://prior.allenai.org/projects/iqa"> / Project Page</a>
          			<a href="https://www.youtube.com/watch?v=pXd3C-1jr98"> / Video</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/matching.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1712.01867"><papertitle>Structured Set Matching Networks for One-Shot Part Labeling</papertitle><a/>
          		<br>
          		<paperauthors>Jonghyun Choi, Jayant Krishnamurthy, Aniruddha Kembhavi, Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>CVPR 2018 &nbsp;<span class='special'>[Spotlight Presentation]</span></papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1712.01867">PDF</a>
          			<a href="https://prior.allenai.org/projects/matching-part-label"> / Project Page</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/gvqa.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1712.00377"><papertitle>Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering</papertitle><a/>
          		<br>
          		<paperauthors>Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>CVPR 2018</papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1712.00377">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/tqa.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="http://ai2-website.s3.amazonaws.com/publications/CVPR17_TQA.pdf"><papertitle>Are You Smarter Than A Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, Hannaneh Hajishirzi</paperauthors>
          		<br>
          		<papervenue>CVPR 2017 &nbsp;<span class='special'>[Spotlight Presentation]<span></papervenue>
          		<p>
          			<a href="http://ai2-website.s3.amazonaws.com/publications/CVPR17_TQA.pdf">PDF</a>
          			<a href="http://prior.allenai.org/projects/tqa/"> / Project Page</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/bidaf.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1611.01603"><papertitle>Bidirectional Attention Flow for Machine Comprehension</papertitle><a/>
          		<br>
          		<paperauthors>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi</paperauthors>
          		<br>
          		<papervenue>ICLR 2017</papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1611.01603">PDF</a>
          			<a href="https://prior.allenai.org/projects/bidaf/"> / Project Page</a>
          			<a href="https://github.com/allenai/bi-att-flow"> / Code</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/diagrams.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1603.07396"><papertitle>A diagram is worth a dozen images</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi</paperauthors>
          		<br>
          		<papervenue>ECCV 2016</papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1603.07396">PDF</a>
          			<a href="http://prior.allenai.org/projects/diagram-understanding"> / Project Page</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/pparsing.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://arxiv.org/abs/1606.07046"><papertitle>Semantic Parsing to Probabilistic Programs for Situated Question Answering</papertitle><a/>
          		<br>
          		<paperauthors>Jayant Krishnamurthy, Oyvind Tafjord, Aniruddha Kembhavi</paperauthors>
          		<br>
          		<papervenue>EMNLP 2016 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          		<p>
          			<a href="https://arxiv.org/abs/1606.07046">PDF</a>
          			<a href="https://github.com/jayantk/jklol/tree/master/src/com/jayantkrish/jklol/experiments/p3"> / Code</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/vehicles.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="papers/Kembhavi_VehicleDetection_PAMI2011.pdf"><papertitle>Vehicle detection using partial least squares</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, David Harwood, Larry S Davis</paperauthors>
          		<br>
          		<papervenue>TPAMI 2011</papervenue>
          		<p>
          			<a href="papers/Kembhavi_VehicleDetection_PAMI2011.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/mln.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="papers/Kembhavi_Scene_Understanding_ECCV2010.pdf"><papertitle>Why did the person cross the road (there)? scene understanding using probabilistic logic models and common sense reasoning</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, Tom Yeh, Larry S Davis</paperauthors>
          		<br>
          		<papervenue>ECCV 2010</papervenue>
          		<p>
          			<a href="papers/Kembhavi_Scene_Understanding_ECCV2010.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/human_object_interactions.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="papers/HumanObjectInteractions_PAMI2009.pdf"><papertitle>Observing human-object interactions: Using spatial and functional compatibility for recognition</papertitle><a/>
          		<br>
          		<paperauthors>Abhinav Gupta, Aniruddha Kembhavi, Larry S. Davis</paperauthors>
          		<br>
          		<papervenue>TPAMI 2009</papervenue>
          		<p>
          			<a href="papers/HumanObjectInteractions_PAMI2009.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/human_detection.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="http://www.umiacs.umd.edu/~lsd/papers/PLS-ICCV09.pdf"><papertitle>Human detection using partial least squares analysis</papertitle><a/>
          		<br>
          		<paperauthors>William Robson Schwartz, Aniruddha Kembhavi, Larry S. Davis</paperauthors>
          		<br>
          		<papervenue>ICCV 2009 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          		<p>
          			<a href="http://www.umiacs.umd.edu/~lsd/papers/PLS-ICCV09.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/imkl.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="http://www.umiacs.umd.edu/~lsd/papers/iccv09imkl_cameraReady.pdf"><papertitle>Scene it or not? incremental multiple kernel learning for object detection</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, Behjat Siddiquie, Roland Miezianko, Scott McCloskey, Larry S. Davis</paperauthors>
          		<br>
          		<papervenue>ICCV 2009</papervenue>
          		<p>
          			<a href="http://www.umiacs.umd.edu/~lsd/papers/iccv09imkl_cameraReady.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/tracking.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="https://hal.inria.fr/file/index/docid/325765/filename/VS2008-Poster-f.pdf"><papertitle>Resource Allocation for Tracking Multiple Targets Using Particle Filters</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, William Robson Schwartz, Larry S. Davis</paperauthors>
          		<br>
          		<papervenue>Workshop on Visual Surveillance held in conjunction with ECCV 2008</papervenue>
          		<p>
          			<a href="https://hal.inria.fr/file/index/docid/325765/filename/VS2008-Poster-f.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

          <tr>
          	<td style="padding:20px;width:15%;vertical-align:middle">
          		<img class="research" src="images/bowerbird.jpg">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:top">
          		<a href="http://www.academia.edu/download/30742527/Kembhavi_Farrell_JDD_WACV2008.pdf"><papertitle>Tracking Down Under: Following the Satin Bowerbird</papertitle><a/>
          		<br>
          		<paperauthors>Aniruddha Kembhavi, Ryan Farrell, Yuancheng Luo, David Jacobs, Ramani Duraiswami, Larry S. Davis</paperauthors>
          		<br>
          		<papervenue>Workshop on Applications of Computer Vision (WACV) 2008 &nbsp;<span class='special'>[Oral Presentation]</span></papervenue>
          		<p>
          			<a href="http://www.academia.edu/download/30742527/Kembhavi_Farrell_JDD_WACV2008.pdf">PDF</a>
          		</p>
          	</td>
          </tr>

        </tbody></table>


        <table class="media" width="100%" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
              <heading>Press Coverage</heading>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#c6edff">
              <b><i>Learning Generalizable Visual Representations via Interactive Gameplay</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/ai-agent-learns-about-the-world-by-gameplay"><img class="media" src="logos/ieeespectrum.png" border=2></a>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#eeeeee">
              <b><i>X-LXMERT: Teaching vision-and-language transformer models to paint</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://www.technologyreview.com/2020/09/25/1008921/ai-allen-institute-generates-images-from-captions/"><img class="media" src="logos/mittechreview.png" border=2></a>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#c6edff">
              <b><i>AllenAct: An open source framework for research in Embodied AI</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://venturebeat.com/2020/08/31/allen-institute-open-sources-allenact-a-framework-for-research-in-embodied-ai/"><img class="media" src="logos/venturebeat.png" border=2></a>
                <a class="media" href="https://www.hackster.io/news/bet-you-can-t-do-that-again-6fc341d01fbd/"><img class="media" src="logos/hackster.png" border=2></a>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#eeeeee">
              <b><i>AI & Creativity</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://www.engadget.com/facebook-ai-choreography-170023727.html"><img class="media" src="logos/engadget.png" border=2></a>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#c6edff">
              <b><i>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://www.technologyreview.com/s/615186/ai-ai2-robots-navigate-world-train-algorithms-challenge/"><img class="media" src="logos/mittechreview.png" border=2></a>
                <a class="media" href="https://www.geekwire.com/tag/robothor-challenge/"><img class="media" src="logos/geekwire.png" border=2></a>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#eeeeee">
              <b><i>Iconary: An AI powered drawing and guessing game</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://www.sciencemag.org/news/2019/02/pictionary-playing-computer-connects-humans-deep-thoughts"><img class="media" src="logos/science.png" border=2></a>
                <a class="media" href="https://www.technologyreview.com/s/612882/an-ai-is-playing-pictionary-to-figure-out-how-the-world-works/"><img class="media" src="logos/mittechreview.png" border=2></a>
                <a class="media" href="https://venturebeat.com/2019/02/05/allen-institute-debuts-ai-that-plays-pictionary-style-games-to-learn-common-sense/"><img class="media" src="logos/venturebeat.png" border=2></a>
                <a class="media" href="https://www.wired.com/story/your-next-game-night-partner-computer-iconary/"><img class="media" src="logos/wired.png" border=2></a>
                <a class="media" href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/pictionary-playing-ai-sketches-the-future-of-human-machine-collaborations"><img class="media" src="logos/ieeespectrum.png" border=2></a>
                <a class="media" href="https://techcrunch.com/2019/02/05/play-iconary-a-simple-drawing-game-that-hides-a-deceptively-deep-ai/"><img class="media" src="logos/techcrunch.png" border=2></a>
                <a class="media" href="https://www.theverge.com/2019/2/5/18211799/pictionary-computers-ai-allen-institute-for-artificial-intelligence-iconary-common-sense"><img class="media" src="logos/theverge.png" border=2></a>
                <a class="media" href="https://www.kuow.org/stories/forget-chess-this-bot-plays-pictionary"><img class="media" src="logos/kuow.png" border=2></a>
                <a class="media" href="https://www.ft.com/content/2038d96e-28e7-11e9-a5ab-ff8ef2b976c7"><img class="media" src="logos/financialtimes.png" border=2></a>
                <a class="media" href="https://techxplore.com/news/2019-02-picture-guessing-game-ai-effort-common.html"><img class="media" src="logos/techxplore.png" border=2></a>
                <a class="media" href="https://www.seattletimes.com/business/technology/what-does-the-seattle-man-who-invented-pictionary-think-about-its-artificial-intelligence-use/"><img class="media" src="logos/seattletimes.png" border=2></a>
                <a class="media" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/artificial-intelligence-deep-thoughts-ai-pictionary-deepmind-a8764581.html"><img class="media" src="logos/independent.png" border=2></a>
            </td>
          </tr>

          <tr>
            <td width="200" bgcolor="#c6edff">
              <b><i>Craft: Scripts to Compositions to Videos</i></b>
            </td>
            <td>
              <p>
                <a class="media" href="https://www.engadget.com/2018/04/15/ai-creates-flintstones-cartoons/"><img class="media" src="logos/engadget.png" border=2></a>
                <a class="media" href="https://gizmodo.com/this-ai-can-automatically-animate-new-flintstones-carto-1825236308"><img class="media" src="logos/gizmodo.png" border=2></a>
                <a class="media" href="https://thenextweb.com/artificial-intelligence/2018/04/11/researchers-trained-an-ai-to-create-flintstones-cartoons/"><img class="media" src="logos/thenextweb.png" border=2></a>
                <a class="media" href="https://www.techtimes.com/articles/225316/20180417/this-ai-makes-flintstones-animation-based-on-text-descriptions-alone.htm"><img class="media" src="logos/techtimes.png" border=2></a>
            </td>
          </tr>



        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Credit</heading>
              <p>This website's source code was adapted from <a href="https://jonbarron.info/">Jon Barron</a>. Thanks for this elegant template Jon!
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
