<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110663561-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-110663561-1');
        </script>
        <!-- Default Statcounter code for My Personal Website https://anikem.github.io/ -->
        <script type="text/javascript">
            var sc_project=12203587;
            var sc_invisible=1;
            var sc_security="67fbccb9";
            var sc_https=1;
            var sc_remove_link=1;
        </script>
        <script type="text/javascript"
            src="https://www.statcounter.com/counter/counter.js" async></script>
        <noscript>
            <div class="statcounter"><img class="statcounter"
                src="https://c.statcounter.com/12203587/0/67fbccb9/1/" alt="Web
                Analytics"></div>
        </noscript>
        <!-- End of Statcounter Code -->
        <title>Aniruddha (Ani) Kembhavi</title>
        <meta name="author" content="Ani Kembhavi">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- CSS  -->
        <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
        <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
        <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
    </head>
    <body>
        <!--
            Navigation Bar
            -->
        <div class="navbar-fixed">
            <nav class="white lighten-5" role="navigation">
                <div class="nav-wrapper container">
                    <a id="logo-container" href="#" class="brand-logo black-text">Ani Kembhavi</a>
                    <ul class="right hide-on-med-and-down">
                        <li><a href="#" class="black-text">Home</a></li>
                        <li><a href="#awards" class="black-text">Awards</a></li>
                        <li><a href="#selectedworks" class="black-text">Selected Works</a></li>
                        <li><a href="#press" class="black-text">Press</a></li>
                    </ul>
                </div>
            </nav>
        </div>
        <!--
            Introduction
            -->
        <div class="container">
            <div class="section">
                <div class="row">
                    <p></p>
                </div>
                <div class="row">
                    <div class="col s12 m9">
                        <h6 class="light">
                            <p>
                                I am the Director of Science Strategy at Wayve AI in London, UK.
                            </p>
                            <p>
                                Previously, I was the Senior Director of Computer Vision at the <a href="http://allenai.org/" target="_blank">Allen Institute for AI (AI2)</a> in Seattle.
                                I am also an Affiliate Associate Professor at the Computer Science & Engineering department at the <a href="https://www.cs.washington.edu/" target="_blank">University of Washington</a>.
                            </p>
                            <p>
                                I enjoy building and deploying AI applications for users to interact with. My work over two decades spans computer vision, robotics and natural language processing.
                            </p>
                            <p>
                                At AI2, my teams spent a considerable amount of time and effort to make our work publicly available and easily accessible,
                                so that others may analyze our work, replicate it and build upon it. This included open sourcing and maintaining software, building live demos as well as collecting
                                and releasing benchmarks to evaluate AI systems.
                            </p>
    
                            <p>
                                I got my Ph.D. at the <a href="http://www.ece.umd.edu/" target="_blank">University of Maryand, College Park</a>
                                under the supervision of <a href="http://www.umiacs.umd.edu/~lsd/" target="_blank">Prof. Larry S. Davis</a>, and also spent five years at Microsoft working in Image and Video Search.
                            </p>

                            <p>
                                I grew up in the city of Pune in the Western part of India -- tinkering with robots, watching cricket and rowing at my college boat club.
                            </p>
                        </h6>
                        <br>
<!--                         <a class="waves-effect waves-light btn indigo lighten-1" href="mailto:anik@allenai.org">Email</a> -->
                        <a class="waves-effect waves-light btn indigo lighten-1" href="https://scholar.google.com/citations?user=JnUevM0AAAAJ">Google Scholar</a>
                        <p>
                    </div>
                    <div class="col s12 m3 center-on-small-only">
                        <div class="image-container">
                            <img class="circle responsive-img" src="images/profile_aniK.jpg">
                        </div>
                    </div>
                </div>
            </div>
            <div class="divider"></div>
        </div>

        <!--
            Achievements
            -->
        <div class="container" id="awards">
            <div class="section">
                <div class="row">
                    <p></p>
                </div>
                <div class="row">
                    <h5>Recent Awards</h5>
                </div>

                <div class="row">
                    <h6 class="light">
                        <blockquotegreen>
                            <b>Best Paper (Honorable Mention) Award for CVPR 2025 : </b><a href="https://allenai.org/blog/molmo">Molmo and PixMo</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Best Paper Award for CVPR 2023 : </b><a href="https://prior.allenai.org/projects/visprog">Visual Programming: Compositional visual reasoning without training</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Outstanding Paper Award for Neurips 2022 : </b><a href="https://procthor.allenai.org/">ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Outstanding Paper Award for CoRL 2024 : </b><a href="https://poliformer.allen.ai/">PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Best Paper Award on Mobile Manipulation for IROS 2024 : </b><a href="https://rchalyang.github.io/HarmonicMM/">Harmonic Mobile Manipulation</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Best Paper Award for ICRA 2024 : </b><a href="https://arxiv.org/abs/2310.08864">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Allen Institute Test Of Time Award 2020 : </b><a href="https://arxiv.org/abs/1611.01603">BiDAF: Bidirectional Attention Flow for Machine Comprehension</a>
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>NVIDIA Pioneer Award 2018 : </b><a href="https://prior.allenai.org/projects/iqa">IQA: Visual Question Answering in Interactive Environments</a>
                        </blockquotegreen>
                    </h6>
                </div>
            </div>
            <div class="divider"></div>
        </div>

        <!--
            Talks
            -->
        <div class="container" id="achievements">
            <div class="section">
                <div class="row">
                    <p></p>
                </div>
                <div class="row">
                    <h5>Recent Talks</h5>
                </div>

                <div class="row">
                    <h6 class="light">
                        <blockquoteblue>
                            <b>Jun 2025 : </b>3 invited talks and 1 panel discussion at CVPR 2025 workshops
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Jun 2024 : </b>5 invited talks and 2 panel discussions at CVPR 2024 workshops
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Mar 2024 : </b>Gerald M. Masson Distinguished Lecture at Johns Hopkins University
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Feb 2024 : </b>University of Washington Robotics Colloquium
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Oct 2023 : </b>Robotics and Computer Vision Panel, 50th Anniversary Celebration for CS @ UMD
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Aug 2023 : </b>AMLD Generative AI Workshop at EPFL, Lausanne Switzerland
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Aug 2023 : </b>Summer School on AI by IIIT Hyderabad
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Aug 2023 : </b>AI + Architecture at Indian Institute for Interior Design, Hubli, India
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Jul 2023 : </b>Talk at University of Maryland, College Park
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Jul 2023 : </b>Talk at Adobe Research
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Jul 2023 : </b>Talk at Amazon
                        </blockquoteblue>
                        <p>
                        <blockquoteblue>
                            <b>Jun 2023 : </b>Best Paper Award talk at CVPR 2023
                        </blockquoteblue>
                    </h6>
                </div>
            </div>
            <div class="divider"></div>
        </div>

        <!--
            Service
            -->
        <div class="container" id="service">
            <div class="section">
                <div class="row">
                    <p></p>
                </div>
                <div class="row">
                    <h5>Service</h5>
                </div>

                <div class="row">
                    <h6 class="light">
                        <blockquote>
                            <b>Program Chair (PC)  : </b>ICCV 2025 <i>(upcoming)</i>
                        </blockquote>
                        <p>
                        <blockquotegreen>
                            <b>Senior Area Chair (SAC) : </b>CVPR 2024
                        </blockquotegreen>
                        <p>
                        <blockquotegreen>
                            <b>Area Chair (AC) : </b>Several past CVPR, ICLR, Neurips and EMNLP conferences
                        </blockquotegreen>
                        <p>
                    </h6>
                </div>
            </div>
            <div class="divider"></div>
        </div>

        <!--
            Selected Works
            -->
        <div class="container" id="selectedworks">
            <div class="section">
                <div class="row">
                    <p></p>
                </div>
                <div class="row">
                    <h5>Selected Works</h5>
                </div>
                <div class="row">
                    <h6 class="light">
                      Here are some selected works from my teams at the Allen Institute for AI.
                    </h6>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/molmo_banner.png">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models</span>
                                <p>Molmo is a family of open state-of-the-art multimodal AI models. Our most powerful model closes the gap between open and
                                  proprietary systems across a wide range of academic benchmarks as well as human evaluations. Our smaller models outperform models 10x their size.
                                <p>While current multimodal models interpret multimodal data and express it in natural language, their full potential remains untapped.
                                  Molmo goes beyond. By learning to point at what it perceives, Molmo enables rich interactions with physical and virtual worlds, empowering the next generation of applications capable of acting and interacting with their environments.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://molmo.allenai.org/blog">Molmo</a>
                            <a href="https://molmo.allenai.org/">Demo</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/unifiedio.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">UNIFIED-IO: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</span>
                                <p>Unified-IO is the first AI model to perform a large and diverse set of AI tasks spanning classical computer vision, image synthesis, vision-and-language,
                                and natural language processing (NLP). It achieved this broad unification by homogenizing every task's input and output into sequences of tokens using universal compressors.
                                <p>Unified-IO 2 scales this to support more modalities, more tasks and larger models. Our 7B parameter model is trained from scratch on
                                1B image-text pairs, 1T text tokens, 180M video clips, 130M interleaved image & text, 3M 3D assets, and 1M robot trajectories.
                                It achieves state-of-the-art performance on the GRIT benchmark and strong results across more than 30 benchmarks in computer vision.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://unified-io.allenai.org/">Unified-IO</a>
                            <a href="https://unified-io-2.allenai.org/">Unified-IO-2</a>
                            <a href="https://github.com/allenai/unified-io-2">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/procthor.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</span>
                                <p>ProcTHOR is a framework for procedural generation of Embodied AI environments. ProcTHOR enables us to sample arbitrarily large datasets of diverse, interactive, customizable,
                                and performant virtual environments to train and evaluate embodied agents.
                                <p>Models trained using only RGB images on ProcTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks
                                for navigation, rearrangement, and arm manipulation.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://procthor.allenai.org/">ProcTHOR</a>
                            <a href="https://github.com/allenai/procthor">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/visprog.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">Visual Programming: Compositional Neuro-Symbolic Visual Reasoning</span>
                                <p>Visual programming is a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions.
                                  It avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs,
                                  which are then executed to get both the solution and a comprehensive and interpretable rationale.
                                  Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://prior.allenai.org/projects/visprog">VISPROG</a>
                            <a href="https://github.com/allenai/visprog">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/objaverse.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">Objaverse and Objaverse-XL: Universes with Over 10M+ Annotated 3D Objects</span>
                                <p>Objaverse and Objaverse-XL are the largest public resources for 3D objects with 1M and 10M high quality assets. They comprise objects from a diverse set of sources,
                                including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts.
                                In less than a year, these datasets have become the de-facto resources for training foundation models for 3D computer vision.
                                <p>We demonstrate the power of the Objaverse asset librraies by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images and achieving strong zero-shot generalization abilities.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://objaverse.allenai.org/objaverse-1.0">Objaverse</a>
                            <a href="https://objaverse.allenai.org/">Objaverse-XL</a>
                            <a href="https://github.com/allenai/objaverse-xl">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/satlas.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">SATLAS: Open Geospatial Data Generated by AI</span>
                                <p>Satlas is a platform for visualizing and downloading global geospatial data products generated by our AI models using satellite images.
                                Currently, it includes marine infrastructure (offshore wind turbines and platforms), renewable energy infrastructure (onshore wind turbines and solar farms), and tree cover.
                                <p>Satlas also contains high resolution imagery on a global scale generated by our super resolution AI models which input freely available low resolution imagery from the Sentinel-2 satellites
                                and produce high fidelity imagery for the entire planet.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://satlas.allen.ai/">SATLAS</a>
                            <a href="https://github.com/allenai/satlas">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/robothor.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">AI2-THOR: An Interactive 3D Simulated Environment to Train Robots</span>
                                <p>AI2-THOR is a simuated environment consisting of near photo-realistic 3D indoor scenes, where AI agents can navigate and interact with objects to perform tasks.
                                It is extensively used in the community to train robot policies using reinforcement learning and imitation learning for tasks such as visual navigation, object manipulation, instruction following, and more.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://ai2thor.allenai.org/">AI2-THOR</a>
                            <a href="https://github.com/allenai/ai2thor">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/spoc.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">SPOC: Imitating Shortest Paths in Simulation For Real World Navigation and Manipulation</span>
                                <p>SPOC is an embodied navigation and manipulation agent trained by imitating shortest-path experts in simulation.
                                SPOC uses no human demonstrations, no reinforcement learning, no depth sensors and makes no assumptions about the target environment.
                                <p>A key factor that enables this surprising result is the scale and diversity of our training data -- made possible by our recent works to procedurally generate simulations via ProcTHOR and HoloDeck and massively scaling up 3D assets via our openly available Objaverse resource.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://spoc-robot.github.io/">SPOC</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/robothor-simreal.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</span>
                                <p>RoboTHOR is a framework to study simulation-to-real transfer for robotics. It consists of simulated environments paired with physical counterparts in the real world.
                                <p>The physical environments are built using modular and movable components, allowing us to host scenes with vastly different layouts within a single physical space.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://ai2thor.allenai.org/robothor">RoboTHOR</a>
                            <a href="https://github.com/allenai/ai2thor">Code</a>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <p></p>
                </div>

                <div class="card">
                    <div class="padcard">
                        <div class="row">
                            <div class="col s12 m4 center-on-small-only">
                                <div class="image-container">
                                    <img class="responsive-img" src="images/bidaf.jpg">
                                </div>
                            </div>
                            <div class="col s12 m8">
                                <span class="card-title blue-text darken-4">BiDaF: Bidirectional Attention Flow for Machine Comprehension</span>
                                <p>BiDaF was an extremely popular and state-of-the-art neural model for machine comprehension before the emergence of the BERT architetcure.
                                <p>It is a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a
                                  query-aware context representation without early summarization.
                            </div>
                        </div>
                        <div class="card-action">
                            <a href="https://github.com/allenai/bi-att-flow">Code</a>
                        </div>
                    </div>
                </div>

            </div>
            <div class="divider"></div>
        </div>

        <!--
            Press
            -->
        <div class="container" id="press">
            <div class="section">
                <div class="row">
                    <p></p>
                </div>
                <div class="row">
                    <h5>Press</h5>
                </div>
                <div class="row">
                    <table class="media" width="100%" border="0" cellspacing="0" cellpadding="10">
                        <tbody>
                          <tr>
                              <td width="200" bgcolor="#eeeeee">
                                  <i>Molmo: A family of open state-of-the-art multimodal AI models</i>
                              </td>
                              <td>
                                  <p>
                                      <a class="media" href="https://techcrunch.com/2024/09/25/ai2s-molmo-shows-open-source-can-meet-and-beat-closed-multimodal-models/"><img class="media" src="logos/techcrunch.png" border=2></a>
                                      <a class="media" href="https://www.wired.com/story/molmo-open-source-multimodal-ai-model-allen-institute-agents/"><img class="media" src="logos/wired.png" border=2></a>
                                      <a class="media" href="https://www.technologyreview.com/2024/09/25/1104465/a-tiny-new-open-source-ai-model-performs-as-well-as-powerful-big-ones/"><img class="media" src="logos/mittechreview.png" border=2></a>
                              </td>
                          </tr>
                          <tr>
                              <td width="200" bgcolor="#c6edff">
                                  <i>Learning Generalizable Visual Representations via Interactive Gameplay</i>
                              </td>
                              <td>
                                  <p>
                                      <a class="media" href="https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/ai-agent-learns-about-the-world-by-gameplay"><img class="media" src="logos/ieeespectrum.png" border=2></a>
                              </td>
                            </tr>
                            <tr>
                                <td width="200" bgcolor="#eeeeee">
                                    <i>X-LXMERT: Teaching vision-and-language transformer models to paint</i>
                                </td>
                                <td>
                                    <p>
                                        <a class="media" href="https://www.technologyreview.com/2020/09/25/1008921/ai-allen-institute-generates-images-from-captions/"><img class="media" src="logos/mittechreview.png" border=2></a>
                                </td>
                            </tr>
                            <tr>
                                <td width="200" bgcolor="#c6edff">
                                    <i>AllenAct: An open source framework for research in Embodied AI</i>
                                </td>
                                <td>
                                    <p>
                                        <a class="media" href="https://venturebeat.com/2020/08/31/allen-institute-open-sources-allenact-a-framework-for-research-in-embodied-ai/"><img class="media" src="logos/venturebeat.png" border=2></a>
                                        <a class="media" href="https://www.hackster.io/news/bet-you-can-t-do-that-again-6fc341d01fbd/"><img class="media" src="logos/hackster.png" border=2></a>
                                </td>
                            </tr>
                            <tr>
                                <td width="200" bgcolor="#eeeeee">
                                    <i>AI & Creativity</i>
                                </td>
                                <td>
                                    <p>
                                        <a class="media" href="https://www.engadget.com/facebook-ai-choreography-170023727.html"><img class="media" src="logos/engadget.png" border=2></a>
                                </td>
                            </tr>
                            <tr>
                                <td width="250" bgcolor="#c6edff">
                                    <i>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</i>
                                </td>
                                <td>
                                    <p>
                                        <a class="media" href="https://www.technologyreview.com/s/615186/ai-ai2-robots-navigate-world-train-algorithms-challenge/"><img class="media" src="logos/mittechreview.png" border=2></a>
                                        <a class="media" href="https://www.geekwire.com/tag/robothor-challenge/"><img class="media" src="logos/geekwire.png" border=2></a>
                                </td>
                            </tr>
                            <tr>
                                <td width="200" bgcolor="#eeeeee">
                                    <i>Iconary: An AI powered drawing and guessing game</i>
                                </td>
                                <td>
                                    <p>
                                        <a class="media" href="https://www.sciencemag.org/news/2019/02/pictionary-playing-computer-connects-humans-deep-thoughts"><img class="media" src="logos/science.png" border=2></a>
                                        <a class="media" href="https://www.technologyreview.com/s/612882/an-ai-is-playing-pictionary-to-figure-out-how-the-world-works/"><img class="media" src="logos/mittechreview.png" border=2></a>
                                        <a class="media" href="https://venturebeat.com/2019/02/05/allen-institute-debuts-ai-that-plays-pictionary-style-games-to-learn-common-sense/"><img class="media" src="logos/venturebeat.png" border=2></a>
                                        <a class="media" href="https://www.wired.com/story/your-next-game-night-partner-computer-iconary/"><img class="media" src="logos/wired.png" border=2></a>
                                        <a class="media" href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/pictionary-playing-ai-sketches-the-future-of-human-machine-collaborations"><img class="media" src="logos/ieeespectrum.png" border=2></a>
                                        <a class="media" href="https://techcrunch.com/2019/02/05/play-iconary-a-simple-drawing-game-that-hides-a-deceptively-deep-ai/"><img class="media" src="logos/techcrunch.png" border=2></a>
                                        <a class="media" href="https://www.theverge.com/2019/2/5/18211799/pictionary-computers-ai-allen-institute-for-artificial-intelligence-iconary-common-sense"><img class="media" src="logos/theverge.png" border=2></a>
                                        <a class="media" href="https://www.kuow.org/stories/forget-chess-this-bot-plays-pictionary"><img class="media" src="logos/kuow.png" border=2></a>
                                        <a class="media" href="https://www.ft.com/content/2038d96e-28e7-11e9-a5ab-ff8ef2b976c7"><img class="media" src="logos/financialtimes.png" border=2></a>
                                        <a class="media" href="https://techxplore.com/news/2019-02-picture-guessing-game-ai-effort-common.html"><img class="media" src="logos/techxplore.png" border=2></a>
                                        <a class="media" href="https://www.seattletimes.com/business/technology/what-does-the-seattle-man-who-invented-pictionary-think-about-its-artificial-intelligence-use/"><img class="media" src="logos/seattletimes.png" border=2></a>
                                        <a class="media" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/artificial-intelligence-deep-thoughts-ai-pictionary-deepmind-a8764581.html"><img class="media" src="logos/independent.png" border=2></a>
                                </td>
                            </tr>
                            <tr>
                                <td width="200" bgcolor="#c6edff">
                                    <i>Craft: Scripts to Compositions to Videos</i>
                                </td>
                                <td>
                                    <p>
                                        <a class="media" href="https://www.engadget.com/2018/04/15/ai-creates-flintstones-cartoons/"><img class="media" src="logos/engadget.png" border=2></a>
                                        <a class="media" href="https://gizmodo.com/this-ai-can-automatically-animate-new-flintstones-carto-1825236308"><img class="media" src="logos/gizmodo.png" border=2></a>
                                        <a class="media" href="https://thenextweb.com/artificial-intelligence/2018/04/11/researchers-trained-an-ai-to-create-flintstones-cartoons/"><img class="media" src="logos/thenextweb.png" border=2></a>
                                        <a class="media" href="https://www.techtimes.com/articles/225316/20180417/this-ai-makes-flintstones-animation-based-on-text-descriptions-alone.htm"><img class="media" src="logos/techtimes.png" border=2></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <br>
            <div class="divider"></div>
        </div>
        <!--
            Footer
            -->
        <footer class="page-footer black">
            <div class="footer-copyright">
                <div class="container">
                    This website was made using <a class="orange-text text-lighten-3" href="http://materializecss.com">Materialize</a>
                </div>
            </div>
        </footer>
        <!--  Scripts-->
        <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
        <script src="js/materialize.js"></script>
        <script src="js/init.js"></script>
    </body>
</html>
